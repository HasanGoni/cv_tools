# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/10_data_processing.hpc_tools.ipynb.

# %% auto 0
__all__ = ['create_job_script', 'create_batches', 'submit_job', 'mock_process_func', 'monitor_jobs', 'get_last_job_id',
           'submit_parallel_jobs']

# %% ../../nbs/10_data_processing.hpc_tools.ipynb 3
from ..core import *
from typing import List, Callable, Any, Dict, Generator, Optional
import subprocess
from pathlib import Path
import tempfile
import os
from tqdm.auto import tqdm
import time
import logging
import pickle
from concurrent.futures import ThreadPoolExecutor

# %% ../../nbs/10_data_processing.hpc_tools.ipynb 4
from unittest.mock import patch, MagicMock

# %% ../../nbs/10_data_processing.hpc_tools.ipynb 5
def create_job_script(
    function_name: str,
    input_path: str,
    output_path: str,
    memory: int = 16000,
    queue: str = "normal",
    python_path: str = "/usr/bin/python",
    additional_args: Dict[str, Any] = None
) -> str:
    """Create a job script for LSF submission.
    
    Args:
        function_name: Name of the Python function to execute
        input_path: Path to input data
        output_path: Path for output data
        memory: Required memory in MB
        queue: LSF queue name
        python_path: Path to Python interpreter
        additional_args: Additional arguments for the function
    
    Returns:
        str: Content of the job script
    """
    script = f"""#!/bin/bash
#BSUB -M {memory}
#BSUB -q {queue}
#BSUB -o %J.out
#BSUB -e %J.err

source ~/.bashrc

{python_path} -c 'import pickle; from {function_name} import process_batch; \
with open("{input_path}", "rb") as f: batch = pickle.load(f); \
process_batch(batch, "{output_path}")
"""
    return script

# %% ../../nbs/10_data_processing.hpc_tools.ipynb 8
def create_batches(files: List[str], batch_size: int) -> Generator[List[str], None, None]:
    """Create batches of files for parallel processing.
    
    Args:
        files: List of file paths
        batch_size: Number of files per batch
    
    Yields:
        List[str]: Batch of file paths
    """
    for i in range(0, len(files), batch_size):
        yield files[i:i + batch_size]

# %% ../../nbs/10_data_processing.hpc_tools.ipynb 13
def submit_job(job_script_path: str) -> bool:
    """Submit a job to LSF.
    
    Args:
        job_script_path: Path to the job script
    
    Returns:
        bool: True if submission was successful
    """
    try:
        result = subprocess.run(["bsub", "<", job_script_path], 
                              capture_output=True, 
                              text=True,
                              shell=True)
        return result.returncode == 0
    except Exception as e:
        logging.error(f"Job submission failed: {e}")
        return False

# %% ../../nbs/10_data_processing.hpc_tools.ipynb 15
# Mock function for testing
def mock_process_func(input_path: str, output_path: str) -> bool:
    return True

# %% ../../nbs/10_data_processing.hpc_tools.ipynb 19
def monitor_jobs(job_ids: List[str]) -> Dict[str, str]:
    """Monitor the status of submitted jobs.
    
    Args:
        job_ids: List of job IDs to monitor
    
    Returns:
        Dict[str, str]: Job IDs mapped to their current status
    """
    try:
        result = subprocess.run(["bjobs"], capture_output=True, text=True)
        lines = result.stdout.strip().split('\n')[1:]  # Skip header
        
        status_dict = {}
        for line in lines:
            parts = line.split()
            if parts[0] in job_ids:
                status_dict[parts[0]] = parts[2]  # STAT column
        
        return status_dict
    except Exception as e:
        logging.error(f"Job monitoring failed: {e}")
        return {}

# %% ../../nbs/10_data_processing.hpc_tools.ipynb 24
def get_last_job_id() -> Optional[str]:
    """Get the ID of the last submitted job.
    
    Returns:
        Optional[str]: Job ID if available, None otherwise
    """
    try:
        result = subprocess.run(["bjobs", "-noheader"], capture_output=True, text=True)
        lines = result.stdout.strip().split('\n')
        if lines and lines[0]:
            return lines[0].split()[0]
        return None
    except Exception:
        return None

# %% ../../nbs/10_data_processing.hpc_tools.ipynb 25
def submit_parallel_jobs(
    function: Callable,
    input_files: List[str],
    output_dir: str,
    batch_size: int = 100,
    memory: int = 16000,
    queue: str = "normal",
    max_concurrent_jobs: int = 1000,
    monitor_interval: int = 60
) -> List[bool]:
    """Submit and monitor parallel jobs for processing files.
    
    Args:
        function: Processing function to apply to each batch
        input_files: List of input file paths
        output_dir: Directory for output files
        batch_size: Number of files per batch
        memory: Memory requirement per job (MB)
        queue: LSF queue name
        max_concurrent_jobs: Maximum number of concurrent jobs
        monitor_interval: Interval for checking job status (seconds)
    
    Returns:
        List[bool]: Success status for each batch submission
    """
    os.makedirs(output_dir, exist_ok=True)
    batches = list(create_batches(input_files, batch_size))
    results = []
    active_jobs = set()
    
    with tqdm(total=len(batches), desc="Submitting jobs") as pbar:
        for i, batch in enumerate(batches):
            # Wait if we have too many active jobs
            while len(active_jobs) >= max_concurrent_jobs:
                time.sleep(monitor_interval)
                status = monitor_jobs(list(active_jobs))
                active_jobs = {jid for jid, stat in status.items() 
                             if stat in ["PEND", "RUN"]}
            
            # Create temporary files for batch data
            with tempfile.NamedTemporaryFile(suffix=".pkl", delete=False) as f:
                pickle.dump(batch, f)
                batch_file = f.name
            
            # Create job script
            job_script = create_job_script(
                function.__module__,
                batch_file,
                os.path.join(output_dir, f"batch_{i}"),
                memory=memory,
                queue=queue
            )
            
            # Write job script
            script_path = f"job_{i}.sh"
            with open(script_path, "w") as f:
                f.write(job_script)
            
            # Submit job
            success = submit_job(script_path)
            results.append(success)
            
            if success:
                job_id = get_last_job_id()
                active_jobs.add(job_id)
            
            pbar.update(1)
            
            # Cleanup
            os.unlink(script_path)
    
    return results
