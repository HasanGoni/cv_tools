{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| default_exp data_processing.hpc_tools"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create HPC related functions\n",
                "> Create HPC(High Performance Cluster) related functions for parallel processing with LSF (bsub)\n",
                "\n",
                "This module provides tools for:\n",
                "1. Creating batch jobs for LSF clusters\n",
                "2. Submitting parallel jobs\n",
                "3. Monitoring job status\n",
                "4. Managing large-scale data processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The autoreload extension is already loaded. To reload it, use:\n",
                        "  %reload_ext autoreload\n"
                    ]
                }
            ],
            "source": [
                "#| hide\n",
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "from cv_tools.core import *\n",
                "from typing import List, Callable, Any, Dict, Generator, Optional\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "import tempfile\n",
                "import os\n",
                "from tqdm.auto import tqdm\n",
                "import time\n",
                "import logging\n",
                "import pickle\n",
                "from concurrent.futures import ThreadPoolExecutor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "from unittest.mock import patch, MagicMock"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "def create_job_script(\n",
                "    function_name: str,\n",
                "    input_path: str,\n",
                "    output_path: str,\n",
                "    memory: int = 16000,\n",
                "    queue: str = \"normal\",\n",
                "    python_path: str = \"/usr/bin/python\",\n",
                "    additional_args: Dict[str, Any] = None\n",
                ") -> str:\n",
                "    \"\"\"Create a job script for LSF submission.\n",
                "    \n",
                "    Args:\n",
                "        function_name: Name of the Python function to execute\n",
                "        input_path: Path to input data\n",
                "        output_path: Path for output data\n",
                "        memory: Required memory in MB\n",
                "        queue: LSF queue name\n",
                "        python_path: Path to Python interpreter\n",
                "        additional_args: Additional arguments for the function\n",
                "    \n",
                "    Returns:\n",
                "        str: Content of the job script\n",
                "    \"\"\"\n",
                "    script = f\"\"\"#!/bin/bash\n",
                "#BSUB -M {memory}\n",
                "#BSUB -q {queue}\n",
                "#BSUB -o %J.out\n",
                "#BSUB -e %J.err\n",
                "\n",
                "source ~/.bashrc\n",
                "\n",
                "{python_path} -c 'import pickle; from {function_name} import process_batch; \\\n",
                "with open(\"{input_path}\", \"rb\") as f: batch = pickle.load(f); \\\n",
                "process_batch(batch, \"{output_path}\")\n",
                "\"\"\"\n",
                "    return script"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| test\n",
                "def test_create_job_script():\n",
                "    #from cv_tools.preprocessing.hpc_tools import create_job_script\n",
                "    \n",
                "    with tempfile.TemporaryDirectory() as tmpdir:\n",
                "        job_script = create_job_script(\n",
                "            function_name=\"process_images\",\n",
                "            input_path=\"/path/to/input\",\n",
                "            output_path=\"/path/to/output\",\n",
                "            memory=16000,\n",
                "            queue=\"normal\",\n",
                "            python_path=\"/usr/bin/python\"\n",
                "        )\n",
                "        \n",
                "        assert \"#!/bin/bash\" in job_script\n",
                "        assert \"#BSUB -M 16000\" in job_script\n",
                "        assert \"#BSUB -q normal\" in job_script\n",
                "        assert \"python -c\" in job_script"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "#!/bin/bash\n",
                        "#BSUB -M 16000\n",
                        "#BSUB -q normal\n",
                        "#BSUB -o %J.out\n",
                        "#BSUB -e %J.err\n",
                        "\n",
                        "source ~/.bashrc\n",
                        "\n",
                        "/usr/bin/python -c 'import pickle; from process_images import process_batch; with open(\"/path/to/input\", \"rb\") as f: batch = pickle.load(f); process_batch(batch, \"/path/to/output\")\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "print(create_job_script(\n",
                "    function_name=\"process_images\",\n",
                "    input_path=\"/path/to/input\",\n",
                "    output_path=\"/path/to/output\",\n",
                "    memory=16000,\n",
                "    queue=\"normal\",\n",
                "    python_path=\"/usr/bin/python\"\n",
                "))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "def create_batches(files: List[str], batch_size: int) -> Generator[List[str], None, None]:\n",
                "    \"\"\"Create batches of files for parallel processing.\n",
                "    \n",
                "    Args:\n",
                "        files: List of file paths\n",
                "        batch_size: Number of files per batch\n",
                "    \n",
                "    Yields:\n",
                "        List[str]: Batch of file paths\n",
                "    \"\"\"\n",
                "    for i in range(0, len(files), batch_size):\n",
                "        yield files[i:i + batch_size]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| test\n",
                "def test_batch_generator():\n",
                "    from cv_tools.preprocessing.hpc_tools import create_batches\n",
                "    \n",
                "    files = [f\"file_{i}.jpg\" for i in range(100)]\n",
                "    batch_size = 10\n",
                "    batches = list(create_batches(files, batch_size))\n",
                "    \n",
                "    assert len(batches) == 10\n",
                "    assert len(batches[0]) == batch_size\n",
                "    assert batches[0][0] == \"file_0.jpg\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_batch_generator()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "def submit_job(job_script_path: str) -> bool:\n",
                "    \"\"\"Submit a job to LSF.\n",
                "    \n",
                "    Args:\n",
                "        job_script_path: Path to the job script\n",
                "    \n",
                "    Returns:\n",
                "        bool: True if submission was successful\n",
                "    \"\"\"\n",
                "    try:\n",
                "        result = subprocess.run([\"bsub\", \"<\", job_script_path], \n",
                "                              capture_output=True, \n",
                "                              text=True,\n",
                "                              shell=True)\n",
                "        return result.returncode == 0\n",
                "    except Exception as e:\n",
                "        logging.error(f\"Job submission failed: {e}\")\n",
                "        return False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| test\n",
                "def test_submit_job():\n",
                "    from cv_tools.preprocessing.hpc_tools import submit_job\n",
                "    \n",
                "    with patch('subprocess.run') as mock_run:\n",
                "        mock_run.return_value = MagicMock(returncode=0)\n",
                "        \n",
                "        success = submit_job(\"test_job.sh\")\n",
                "        assert success\n",
                "        mock_run.assert_called_once()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "# Mock function for testing\n",
                "def mock_process_func(input_path: str, output_path: str) -> bool:\n",
                "    return True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| test\n",
                "def test_parallel_job_submission():\n",
                "    from cv_tools.preprocessing.hpc_tools import submit_parallel_jobs\n",
                "    \n",
                "    with tempfile.TemporaryDirectory() as tmpdir:\n",
                "        input_files = [f\"{tmpdir}/input_{i}.jpg\" for i in range(10)]\n",
                "        output_dir = f\"{tmpdir}/output\"\n",
                "        \n",
                "        # Create dummy input files\n",
                "        for f in input_files:\n",
                "            Path(f).touch()\n",
                "        \n",
                "        with patch('subprocess.run') as mock_run:\n",
                "            mock_run.return_value = MagicMock(returncode=0)\n",
                "            \n",
                "            results = submit_parallel_jobs(\n",
                "                function=mock_process_func,\n",
                "                input_files=input_files,\n",
                "                output_dir=output_dir,\n",
                "                batch_size=2,\n",
                "                memory=16000,\n",
                "                queue=\"normal\"\n",
                "            )\n",
                "            \n",
                "            assert len(results) == 5  # 10 files / 2 batch_size = 5 jobs\n",
                "            assert all(results)  # All jobs should be submitted successfully"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "def monitor_jobs(job_ids: List[str]) -> Dict[str, str]:\n",
                "    \"\"\"Monitor the status of submitted jobs.\n",
                "    \n",
                "    Args:\n",
                "        job_ids: List of job IDs to monitor\n",
                "    \n",
                "    Returns:\n",
                "        Dict[str, str]: Job IDs mapped to their current status\n",
                "    \"\"\"\n",
                "    try:\n",
                "        result = subprocess.run([\"bjobs\"], capture_output=True, text=True)\n",
                "        lines = result.stdout.strip().split('\\n')[1:]  # Skip header\n",
                "        \n",
                "        status_dict = {}\n",
                "        for line in lines:\n",
                "            parts = line.split()\n",
                "            if parts[0] in job_ids:\n",
                "                status_dict[parts[0]] = parts[2]  # STAT column\n",
                "        \n",
                "        return status_dict\n",
                "    except Exception as e:\n",
                "        logging.error(f\"Job monitoring failed: {e}\")\n",
                "        return {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| test\n",
                "def test_job_monitoring():\n",
                "    from cv_tools.preprocessing.hpc_tools import monitor_jobs\n",
                "    \n",
                "    with patch('subprocess.run') as mock_run:\n",
                "        mock_run.return_value = MagicMock(\n",
                "            returncode=0,\n",
                "            stdout=\"JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME\\n123     user    RUN   normal     host1       host2       job1\"\n",
                "        )\n",
                "        \n",
                "        job_ids = [\"123\"]\n",
                "        status = monitor_jobs(job_ids)\n",
                "        assert status[\"123\"] == \"RUN\" "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "def get_last_job_id() -> Optional[str]:\n",
                "    \"\"\"Get the ID of the last submitted job.\n",
                "    \n",
                "    Returns:\n",
                "        Optional[str]: Job ID if available, None otherwise\n",
                "    \"\"\"\n",
                "    try:\n",
                "        result = subprocess.run([\"bjobs\", \"-noheader\"], capture_output=True, text=True)\n",
                "        lines = result.stdout.strip().split('\\n')\n",
                "        if lines and lines[0]:\n",
                "            return lines[0].split()[0]\n",
                "        return None\n",
                "    except Exception:\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "def submit_parallel_jobs(\n",
                "    function: Callable,\n",
                "    input_files: List[str],\n",
                "    output_dir: str,\n",
                "    batch_size: int = 100,\n",
                "    memory: int = 16000,\n",
                "    queue: str = \"normal\",\n",
                "    max_concurrent_jobs: int = 1000,\n",
                "    monitor_interval: int = 60\n",
                ") -> List[bool]:\n",
                "    \"\"\"Submit and monitor parallel jobs for processing files.\n",
                "    \n",
                "    Args:\n",
                "        function: Processing function to apply to each batch\n",
                "        input_files: List of input file paths\n",
                "        output_dir: Directory for output files\n",
                "        batch_size: Number of files per batch\n",
                "        memory: Memory requirement per job (MB)\n",
                "        queue: LSF queue name\n",
                "        max_concurrent_jobs: Maximum number of concurrent jobs\n",
                "        monitor_interval: Interval for checking job status (seconds)\n",
                "    \n",
                "    Returns:\n",
                "        List[bool]: Success status for each batch submission\n",
                "    \"\"\"\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    batches = list(create_batches(input_files, batch_size))\n",
                "    results = []\n",
                "    active_jobs = set()\n",
                "    \n",
                "    with tqdm(total=len(batches), desc=\"Submitting jobs\") as pbar:\n",
                "        for i, batch in enumerate(batches):\n",
                "            # Wait if we have too many active jobs\n",
                "            while len(active_jobs) >= max_concurrent_jobs:\n",
                "                time.sleep(monitor_interval)\n",
                "                status = monitor_jobs(list(active_jobs))\n",
                "                active_jobs = {jid for jid, stat in status.items() \n",
                "                             if stat in [\"PEND\", \"RUN\"]}\n",
                "            \n",
                "            # Create temporary files for batch data\n",
                "            with tempfile.NamedTemporaryFile(suffix=\".pkl\", delete=False) as f:\n",
                "                pickle.dump(batch, f)\n",
                "                batch_file = f.name\n",
                "            \n",
                "            # Create job script\n",
                "            job_script = create_job_script(\n",
                "                function.__module__,\n",
                "                batch_file,\n",
                "                os.path.join(output_dir, f\"batch_{i}\"),\n",
                "                memory=memory,\n",
                "                queue=queue\n",
                "            )\n",
                "            \n",
                "            # Write job script\n",
                "            script_path = f\"job_{i}.sh\"\n",
                "            with open(script_path, \"w\") as f:\n",
                "                f.write(job_script)\n",
                "            \n",
                "            # Submit job\n",
                "            success = submit_job(script_path)\n",
                "            results.append(success)\n",
                "            \n",
                "            if success:\n",
                "                job_id = get_last_job_id()\n",
                "                active_jobs.add(job_id)\n",
                "            \n",
                "            pbar.update(1)\n",
                "            \n",
                "            # Cleanup\n",
                "            os.unlink(script_path)\n",
                "    \n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "def noop(x):return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_image(\n",
                "        input_path: str, \n",
                "        output_path: str,\n",
                "        process_func=noop) -> bool:\n",
                "    try:\n",
                "        # Your image processing logic here\n",
                "        img = cv2.imread(input_path)\n",
                "        processed = process_func(img)\n",
                "        cv2.imwrite(output_path, processed)\n",
                "        return True\n",
                "    except Exception as e:\n",
                "        logging.error(f\"Error processing {input_path}: {e}\")\n",
                "        return False\n",
                "\t\n",
                "\n",
                "# Get list of files\n",
                "input_files = Path(\"/path/to/input\").ls(file_exts=['.jpg'])\n",
                "output_dir = \"/path/to/output\"\n",
                "\n",
                "# Submit jobs\n",
                "results = submit_parallel_jobs(\n",
                "    function=process_image,\n",
                "    input_files=input_files,\n",
                "    output_dir=output_dir,\n",
                "    batch_size=100,  # Process 100 images per job\n",
                "    memory=16000,    # 16GB memory per job\n",
                "    queue=\"normal\",  # LSF queue name\n",
                "    max_concurrent_jobs=1000,  # Maximum concurrent jobs\n",
                "    monitor_interval=60  # Check job status every 60 seconds\n",
                ")\n",
                "\n",
                "# Mock LSF commands for local testing\n",
                "def mock_bsub(script_path):\n",
                "    with open(script_path) as f:\n",
                "        script = f.read()\n",
                "    # Execute the Python command directly\n",
                "    cmd = script.split(\"python -c\")[1].strip(\"'\")\n",
                "    subprocess.run([\"python\", \"-c\", cmd], check=True)\n",
                "\n",
                "# Test with a small batch\n",
                "test_files = input_files[:10]\n",
                "with patch('subprocess.run', side_effect=mock_bsub):\n",
                "    results = submit_parallel_jobs(\n",
                "        function=process_image,\n",
                "        input_files=test_files,\n",
                "        output_dir=\"test_output\",\n",
                "        batch_size=2\n",
                "    )\n",
                "\n",
                "# Check job status\n",
                "job_ids = [\"123\", \"124\", \"125\"]\n",
                "status = monitor_jobs(job_ids)\n",
                "for job_id, state in status.items():\n",
                "    print(f\"Job {job_id}: {state}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<unknown>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
                        "<unknown>:5: SyntaxWarning: invalid escape sequence '\\s'\n"
                    ]
                }
            ],
            "source": [
                "#| hide\n",
                "import nbdev; nbdev.nbdev_export('10_data_processing.hpc_tools.ipynb')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
