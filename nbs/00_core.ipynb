{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from fastcore.all import *\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Union, Dict, List, NewType\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.ndimage import (\n",
    "    label, binary_dilation, binary_erosion,label,\n",
    "    )\n",
    "from skimage.color import label2rgb\n",
    "from skimage.restoration import denoise_nl_means, estimate_sigma, denoise_tv_chambolle\n",
    "from skimage import img_as_float\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "OpenCvImage = NewType('OpenCvImage', np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "get_name_ = np.vectorize(lambda x: x.name)\n",
    "dpi = mpl.rcParams['figure.dpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def filter_(self:Path, name_part:str):\n",
    "    'filter based on name_part in file' \n",
    "    return  L(filter(lambda x: name_part in x.name, self.ls()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def label_mask(\n",
    "        mask:np.ndarray\n",
    "        )->np.ndarray:\n",
    "    \"Label connected components in a binary mask\"\n",
    "    try: \n",
    "        labels, num_labels = label(mask)\n",
    "        return labels, num_labels\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_labeled_mask(\n",
    "        msk_path):\n",
    "    \n",
    "    msk_img = cv2.imread(str(msk_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    labels, num_labels = label_mask(msk_img)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.imshow(labels)\n",
    "    # Annotate each object with its label number\n",
    "    ax.set_facecolor('black')\n",
    "    for i in range(1, num_labels + 1):\n",
    "        y, x = np.where(labels == i)\n",
    "        centroid = (x.mean(), y.mean())\n",
    "        ax.text(centroid[0], centroid[1], str(i), color='white', ha='center', va='center', fontsize=12)\n",
    "    plt.axis('off')\n",
    "    return labels, num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_new_mask(\n",
    "                     new_lb, # labeled masks\n",
    "                     new_mask_path, # path to save the new mask\n",
    "                     fn # name of the image\n",
    "                     ):\n",
    "    'Write mask to new_mask_path'\n",
    "    new_lbl = new_lb.astype(np.uint8)\n",
    "    new_lbl = np.where(new_lbl > .9, 255, 0)\n",
    "    cv2.imwrite(f'{new_mask_path}/{fn}', new_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_object_from_mask(\n",
    "                           mask:np.ndarray,\n",
    "                           object_id_list:List[int]\n",
    "                           ):\n",
    "    \"\"\"Remove object from mask.\"\"\"\n",
    "\n",
    "    for i in object_id_list:\n",
    "        mask[mask == i] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_img(\n",
    "    im_path:Union[str, Path],\n",
    "    cv:bool=True,\n",
    "    gray:bool=True\n",
    "    ):\n",
    "    'Read image from name could be open cv or pil image'\n",
    "    if cv:\n",
    "        if gray:\n",
    "            return cv2.imread(f'{im_path}', cv2.IMREAD_GRAYSCALE)\n",
    "        else:\n",
    "            return cv2.cvtColor(cv2.imread(f'{im_path}'), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    else:\n",
    "        img = Image.open(im_path)\n",
    "        if gray:\n",
    "            return img.convert('L')\n",
    "        else:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def blur_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        ks:int=3, # kernel size\n",
    "        ) -> np.ndarray:\n",
    "    'Blur image'\n",
    "    return cv2.GaussianBlur(img, (ks, ks), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def median_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        ks:int=3, # kernel size\n",
    "        ) -> np.ndarray:\n",
    "    'Median image'\n",
    "    return cv2.medianBlur(img, ks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def canny_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        th1:int=50, #threshold1\n",
    "        th2:int=150, #threshold2\n",
    "        ) -> np.ndarray:\n",
    "    'Canny image'\n",
    "    return cv2.Canny(img, th1, th2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sharpen_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        ) -> np.ndarray:\n",
    "    'Sharpen image'\n",
    "    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def bilateral_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        d:int=9, # diameter of each pixel neighborhood\n",
    "        sc:int=75, # sigma color\n",
    "        ss:int=75, # sigma space\n",
    "        ) -> np.ndarray:\n",
    "    'Bilateral image'\n",
    "    return cv2.bilateralFilter(img, d, sc, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def nlm_filter_(\n",
    "    img:np.ndarray,  #image\n",
    "    h:float=1.15,\n",
    "    fast_mode:bool=True,\n",
    "    patch_size:int=5,\n",
    "    patch_distance:int=3\n",
    "\n",
    "  ):\n",
    "  'Apply Non local means denoising filter to image'\n",
    "  img = img.astype(np.float)\n",
    "  sigma_est = np.mean(\n",
    "    estimate_sigma(img) \n",
    "    )\n",
    "  denoise_img = denoise_nl_means(\n",
    "    img, \n",
    "    h=h* sigma_est, \n",
    "    fast_mode=fast_mode,\n",
    "    patch_size=patch_size,\n",
    "    patch_distance=patch_distance,\n",
    "    )\n",
    "  return denoise_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdenoise_tv_chambolle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0002\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_num_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mchannel_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Perform total variation denoising in nD.\n",
      "\n",
      "Given :math:`f`, a noisy image (input data),\n",
      "total variation denoising (also known as total variation regularization)\n",
      "aims to find an image :math:`u` with less total variation than :math:`f`,\n",
      "under the constraint that :math:`u` remain similar to :math:`f`.\n",
      "This can be expressed by the Rudin--Osher--Fatemi (ROF) minimization\n",
      "problem:\n",
      "\n",
      ".. math::\n",
      "\n",
      "    \\min_{u} \\sum_{i=0}^{N-1} \\left( \\left| \\nabla{u_i} \\right| + \\frac{\\lambda}{2}(f_i - u_i)^2 \\right)\n",
      "\n",
      "where :math:`\\lambda` is a positive parameter.\n",
      "The first term of this cost function is the total variation;\n",
      "the second term represents data fidelity. As :math:`\\lambda \\to 0`,\n",
      "the total variation term dominates, forcing the solution to have smaller\n",
      "total variation, at the expense of looking less like the input data.\n",
      "\n",
      "This code is an implementation of the algorithm proposed by Chambolle\n",
      "in [1]_ to solve the ROF problem.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "image : ndarray\n",
      "    Input image to be denoised. If its dtype is not float, it gets\n",
      "    converted with :func:`~.img_as_float`.\n",
      "weight : float, optional\n",
      "    Denoising weight. It is equal to :math:`\\frac{1}{\\lambda}`. Therefore,\n",
      "    the greater the `weight`, the more denoising (at the expense of\n",
      "    fidelity to `image`).\n",
      "eps : float, optional\n",
      "    Tolerance :math:`\\varepsilon > 0` for the stop criterion (compares to\n",
      "    absolute value of relative difference of the cost function :math:`E`):\n",
      "    The algorithm stops when :math:`|E_{n-1} - E_n| < \\varepsilon * E_0`.\n",
      "max_num_iter : int, optional\n",
      "    Maximal number of iterations used for the optimization.\n",
      "channel_axis : int or None, optional\n",
      "    If ``None``, the image is assumed to be grayscale (single-channel).\n",
      "    Otherwise, this parameter indicates which axis of the array corresponds\n",
      "    to channels.\n",
      "\n",
      "    .. versionadded:: 0.19\n",
      "       ``channel_axis`` was added in 0.19.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "u : ndarray\n",
      "    Denoised image.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Make sure to set the `channel_axis` parameter appropriately for color\n",
      "images.\n",
      "\n",
      "The principle of total variation denoising is explained in [2]_.\n",
      "It is about minimizing the total variation of an image,\n",
      "which can be roughly described as\n",
      "the integral of the norm of the image gradient. Total variation\n",
      "denoising tends to produce cartoon-like images, that is,\n",
      "piecewise-constant images.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "denoise_tv_bregman : Perform total variation denoising using split-Bregman\n",
      "    optimization.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] A. Chambolle, An algorithm for total variation minimization and\n",
      "       applications, Journal of Mathematical Imaging and Vision,\n",
      "       Springer, 2004, 20, 89-97.\n",
      ".. [2] https://en.wikipedia.org/wiki/Total_variation_denoising\n",
      "\n",
      "Examples\n",
      "--------\n",
      "2D example on astronaut image:\n",
      "\n",
      ">>> from skimage import color, data\n",
      ">>> img = color.rgb2gray(data.astronaut())[:50, :50]\n",
      ">>> rng = np.random.default_rng()\n",
      ">>> img += 0.5 * img.std() * rng.standard_normal(img.shape)\n",
      ">>> denoised_img = denoise_tv_chambolle(img, weight=60)\n",
      "\n",
      "3D example on synthetic data:\n",
      "\n",
      ">>> x, y, z = np.ogrid[0:20, 0:20, 0:20]\n",
      ">>> mask = (x - 22)**2 + (y - 20)**2 + (z - 17)**2 < 8**2\n",
      ">>> mask = mask.astype(float)\n",
      ">>> rng = np.random.default_rng()\n",
      ">>> mask += 0.2 * rng.standard_normal(mask.shape)\n",
      ">>> res = denoise_tv_chambolle(mask, weight=100)\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\goni\\appdata\\local\\anaconda3\\lib\\site-packages\\skimage\\restoration\\_denoise.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "denoise_tv_chambolle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tv_filter_(\n",
    "    img:np.ndarray,  #image\n",
    "    weight:float=0.1, # greater the weight, more denoising, but with less details\n",
    "    max_num_iter:int=200,\n",
    "\n",
    "    ):\n",
    "    'Apply total variation denoising filter to image'\n",
    "    #img = img.astype(np.float)\n",
    "    #d_img = denoise_tv_chambolle(\n",
    "        #img, \n",
    "        #weight=0.1, \n",
    "        #eps=0.0002,\n",
    "        #max_num_iter=max_num_iter,\n",
    "        #)\n",
    "    dst = cv2.fastNlMeansDenoising(np.clip(img,0,255), None, 4, 7, 35)\n",
    "    return dst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev\n",
    "nbdev.nbdev_export('00_core.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        filters_:List[str] # opencv filters\n",
    "        ) -> np.ndarray:\n",
    "\n",
    "    'Apply filter on image'\n",
    "\n",
    "    ops = {\n",
    "        'blur': blur_img_,\n",
    "        'med': median_img_,\n",
    "        'canny': canny_img_,\n",
    "        'sharpen': sharpen_img_,\n",
    "        'nlm': nlm_filter_,\n",
    "        'tv': tv_filter_,\n",
    "        'bilateral': bilateral_img_,\n",
    "    }\n",
    "    for fl in filters_:\n",
    "        if fl in ops:\n",
    "            img = ops[fl](img)\n",
    "        else:\n",
    "            print(f'Filter {fl} not found')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_text_to_image(\n",
    "                        img: np.ndarray, \n",
    "                        text: str, \n",
    "                        position: Tuple[int, int]=None,\n",
    "                        font=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        font_scale: float=1, \n",
    "                        color: Tuple[int, int, int]=(255, 255, 255), \n",
    "                        thickness: int=2):\n",
    "    \"\"\"\n",
    "    Add text to an image.\n",
    "\n",
    "    Parameters:\n",
    "    img (np.ndarray): The image to add text to.\n",
    "    text (str): The text to add.\n",
    "    position (Tuple[int, int]): The position where the text should be added.\n",
    "    font: The font to use. Default is cv2.FONT_HERSHEY_SIMPLEX.\n",
    "    font_scale (float): The font scale. Default is 1.\n",
    "    color (Tuple[int, int, int]): The color of the text in BGR format. Default is white.\n",
    "    thickness (int): The thickness of the text. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The image with the text added.\n",
    "    \"\"\"\n",
    "    if position is None:\n",
    "        # Set the position to a small offset from the top left corner\n",
    "        position = (10, 30)  # 10 pixels from the left, 30 pixels from the top\n",
    "    img_with_text = cv2.putText(img.copy(), text, position, font, font_scale, color, thickness)\n",
    "    return img_with_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def denoise_img(\n",
    "    img:np.ndarray,  #image\n",
    "    filters_:List[str], # opencv filters\n",
    "    h:float=1.15,\n",
    "    fast_mode:bool=True,\n",
    "    patch_size:int=5,\n",
    "    patch_distance:int=3,\n",
    "    multichannel:bool=False,\n",
    "    weight:float=0.1, # greater the weight, more denoising, but with less details\n",
    "    n_iter_max:int=200,\n",
    "    ):\n",
    "    'Apply filter on image'\n",
    "\n",
    "    ops = {\n",
    "        'blur': blur_img_,\n",
    "        'med': median_img_,\n",
    "        'canny': canny_img_,\n",
    "        'sharpen': sharpen_img_,\n",
    "        'nlm': nlm_filter_,\n",
    "        'tv': tv_filter_,\n",
    "        'bilateral': bilateral_img_,\n",
    "    }\n",
    "    images_= []\n",
    "    for fl in filters_:\n",
    "        if fl in ops:\n",
    "            img_ = ops[fl](img)\n",
    "            image_ = add_text_to_image(img_, fl)\n",
    "            images_.append(image_)\n",
    "\n",
    "        else:\n",
    "            print(f'Filter {fl} not found')\n",
    "    return show_(np.concatenate(images_, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n",
      "\n",
      "Join a sequence of arrays along an existing axis.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "a1, a2, ... : sequence of array_like\n",
      "    The arrays must have the same shape, except in the dimension\n",
      "    corresponding to `axis` (the first, by default).\n",
      "axis : int, optional\n",
      "    The axis along which the arrays will be joined.  If axis is None,\n",
      "    arrays are flattened before use.  Default is 0.\n",
      "out : ndarray, optional\n",
      "    If provided, the destination to place the result. The shape must be\n",
      "    correct, matching that of what concatenate would have returned if no\n",
      "    out argument were specified.\n",
      "dtype : str or dtype\n",
      "    If provided, the destination array will have this dtype. Cannot be\n",
      "    provided together with `out`.\n",
      "\n",
      "    .. versionadded:: 1.20.0\n",
      "\n",
      "casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
      "    Controls what kind of data casting may occur. Defaults to 'same_kind'.\n",
      "\n",
      "    .. versionadded:: 1.20.0\n",
      "\n",
      "Returns\n",
      "-------\n",
      "res : ndarray\n",
      "    The concatenated array.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "ma.concatenate : Concatenate function that preserves input masks.\n",
      "array_split : Split an array into multiple sub-arrays of equal or\n",
      "              near-equal size.\n",
      "split : Split array into a list of multiple sub-arrays of equal size.\n",
      "hsplit : Split array into multiple sub-arrays horizontally (column wise).\n",
      "vsplit : Split array into multiple sub-arrays vertically (row wise).\n",
      "dsplit : Split array into multiple sub-arrays along the 3rd axis (depth).\n",
      "stack : Stack a sequence of arrays along a new axis.\n",
      "block : Assemble arrays from blocks.\n",
      "hstack : Stack arrays in sequence horizontally (column wise).\n",
      "vstack : Stack arrays in sequence vertically (row wise).\n",
      "dstack : Stack arrays in sequence depth wise (along third dimension).\n",
      "column_stack : Stack 1-D arrays as columns into a 2-D array.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "When one or more of the arrays to be concatenated is a MaskedArray,\n",
      "this function will return a MaskedArray object instead of an ndarray,\n",
      "but the input masks are *not* preserved. In cases where a MaskedArray\n",
      "is expected as input, use the ma.concatenate function from the masked\n",
      "array module instead.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> a = np.array([[1, 2], [3, 4]])\n",
      ">>> b = np.array([[5, 6]])\n",
      ">>> np.concatenate((a, b), axis=0)\n",
      "array([[1, 2],\n",
      "       [3, 4],\n",
      "       [5, 6]])\n",
      ">>> np.concatenate((a, b.T), axis=1)\n",
      "array([[1, 2, 5],\n",
      "       [3, 4, 6]])\n",
      ">>> np.concatenate((a, b), axis=None)\n",
      "array([1, 2, 3, 4, 5, 6])\n",
      "\n",
      "This function will not preserve masking of MaskedArray inputs.\n",
      "\n",
      ">>> a = np.ma.arange(3)\n",
      ">>> a[1] = np.ma.masked\n",
      ">>> b = np.arange(2, 5)\n",
      ">>> a\n",
      "masked_array(data=[0, --, 2],\n",
      "             mask=[False,  True, False],\n",
      "       fill_value=999999)\n",
      ">>> b\n",
      "array([2, 3, 4])\n",
      ">>> np.concatenate([a, b])\n",
      "masked_array(data=[0, 1, 2, 2, 3, 4],\n",
      "             mask=False,\n",
      "       fill_value=999999)\n",
      ">>> np.ma.concatenate([a, b])\n",
      "masked_array(data=[0, --, 2, 2, 3, 4],\n",
      "             mask=[False,  True, False, False, False, False],\n",
      "       fill_value=999999)\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "np.concatenate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_(\n",
    "    im_path:Union[str, np.ndarray],\n",
    "\tcmap:str='gray'\n",
    "    ):\n",
    "    'Showing an image could be image or str, '\n",
    "\n",
    "    if isinstance(im_path,str) or isinstance(im_path, Path):\n",
    "        im = Image.open(im_path)\n",
    "        fig, ax = plt.subplots(figsize= (im.size[0]/dpi, im.size[1]/dpi))\n",
    "        ax.imshow(im, cmap=cmap)\n",
    "        ax.axis('off')\n",
    "    elif isinstance(im_path, list):\n",
    "            for i in im_path:\n",
    "                if isinstance(i, str) or isinstance(i, Path):\n",
    "                    im = Image.open(i)\n",
    "                    h, w = im.size\n",
    "                    fig, ax = plt.subplots(figsize= (w/dpi, h/dpi))\n",
    "                    ax.imshow(i, cmap=cmap)\n",
    "                    ax.axis('off')\n",
    "                elif isinstance(i, np.ndarray):\n",
    "                    h, w = i.shape[0], i.shape[1]\n",
    "                    fig, ax = plt.subplots(figsize= (w/dpi, h/dpi))\n",
    "                    ax.imshow(i, cmap=cmap)\n",
    "                    ax.axis('off')\n",
    "                    \n",
    "                    \n",
    "    else: \n",
    "        h, w = im_path.shape[0], im_path.shape[1]\n",
    "        fig, ax = plt.subplots(figsize= (w/dpi, h/dpi))\n",
    "        im = im_path\n",
    "        ax.imshow(im,cmap=cmap)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#def normalize(\n",
    "              #image:Union[np.ndarray, tf.Tensor], \n",
    "              #min=0):\n",
    "    #def _normalize(im):\n",
    "        #img = tf.cast(im, tf.float32)\n",
    "        #return img / 255.0\n",
    "\n",
    "    #if min == 0:\n",
    "        #return _normalize(image)\n",
    "    #else:\n",
    "        #return (_normalize(image) * 2.0) -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def center_crop(\n",
    "        image:Image,# not open cv but PIL Image \n",
    "        desired_width:int=1632,\n",
    "        desired_height:int=1152,\n",
    "        height_offset:int=-50,\n",
    "        width_offset:int=-70,\n",
    "        cv:bool=True\n",
    "        ):\n",
    "    # Get the current size of the image\n",
    "    width, height = image.size\n",
    "\n",
    "    # Calculate the coordinates of the center of the image\n",
    "    center_x = (width // 2 + (width_offset))\n",
    "    center_y = (height // 2 + (height_offset))\n",
    "\n",
    "    # Calculate the coordinates of the top-left corner of the crop\n",
    "    left = center_x - desired_width // 2\n",
    "    top = center_y - desired_height // 2\n",
    "\n",
    "    # Calculate the coordinates of the bottom-right corner of the crop\n",
    "    right = center_x + desired_width // 2\n",
    "    bottom = center_y + desired_height // 2\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "    #print(f'cropped_image size = {cropped_image.size}')\n",
    "    if cv:\n",
    "        return np.array(cropped_image)\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def overlay_mask(\n",
    "        im_path:Union[str,Path],\n",
    "        msk_path:Union[str,Path], \n",
    "        overlay_clr:Tuple[int, int, int]=(0, 1, 0), # color\n",
    "        scale:int=1, # to scale the image \n",
    "        alpha:float=0.5, # visibility\n",
    "        ):\n",
    "    'Creaete a overlay image from image and mask'\n",
    "    # Read the grayscale image\n",
    "    gray_img = cv2.imread(f'{im_path}', cv2.IMREAD_GRAYSCALE)\n",
    "    if gray_img is None:\n",
    "        raise ValueError(\"Could not read the grayscale image\")\n",
    "\n",
    "    # Read the mask image\n",
    "    mask_img = cv2.imread(f'{msk_path}', cv2.IMREAD_GRAYSCALE)\n",
    "    mask_img = mask_img.astype(bool)\n",
    "    if mask_img is None:\n",
    "        raise ValueError(\"Could not read the mask image\")\n",
    "\n",
    "    # Check if dimensions of both images are the same\n",
    "    if gray_img.shape != mask_img.shape:\n",
    "        raise ValueError(\"Dimensions of grayscale image and mask do not match\")\n",
    "\n",
    "    # Convert image to 3 channels\n",
    "    rgb_img = np.stack([gray_img]*3, axis=-1)/255\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(rgb_img)\n",
    "\n",
    "    clrd_overlay = np.zeros_like(rgb_img)\n",
    "    clrd_overlay[mask_img]=overlay_clr\n",
    "    ax.imshow(clrd_overlay, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def overlay_mask_border_on_image(\n",
    "        im_path: Union[Path, str],\n",
    "        msk_path: Union[Path, str],\n",
    "        save_new_img_path:Union[Path,str]=None,\n",
    "        save_overlay_img_path:Union[Path,str]=None,\n",
    "        new_img:Union[List, np.ndarray, None] = None,\n",
    "        scale_:int=1,\n",
    "        border_color: Tuple[int, int, int] = (0, 1, 0),\n",
    "        border_width: int = 1,\n",
    "        show_:bool=False):\n",
    "    \"\"\"\n",
    "    Overlays the border of a binary mask on a grayscale image and displays the result using matplotlib.\n",
    "\n",
    "    Args:\n",
    "    image (numpy.ndarray): Grayscale image.\n",
    "    mask (numpy.ndarray): Binary mask of the same size as the image.\n",
    "    border_color (tuple): RGB color for the mask border in the range [0, 1].\n",
    "    border_width (int): Width of the border.\n",
    "\n",
    "    Returns:\n",
    "    None: The function displays a plot.\n",
    "    \"\"\"\n",
    "    name_ = Path(im_path).name\n",
    "    gray_img = cv2.imread(f'{im_path}', cv2.IMREAD_GRAYSCALE)\n",
    "    if gray_img is None:\n",
    "        raise ValueError(\"Could not read the grayscale image\")\n",
    "\n",
    "    # Read the mask image\n",
    "    mask_img = cv2.imread(f'{msk_path}', cv2.IMREAD_GRAYSCALE)\n",
    "    mask_img = mask_img.astype(bool)\n",
    "    if mask_img is None:\n",
    "        raise ValueError(\"Could not read the mask image\")\n",
    "\n",
    "    # Check if dimensions of both images are the same\n",
    "    if gray_img.shape != mask_img.shape:\n",
    "        raise ValueError(\"Dimensions of grayscale image and mask do not match\")\n",
    "    # Ensure the mask is boolean\n",
    "\n",
    "    # Find the borders of the mask\n",
    "    dilated_mask = binary_dilation(mask_img, iterations=border_width)\n",
    "    eroded_mask = binary_erosion(mask_img, iterations=border_width)\n",
    "    border = dilated_mask & ~eroded_mask\n",
    "\n",
    "    # Convert grayscale image to RGB\n",
    "    rgb_image = np.stack([gray_img]*3, axis=-1) / \\\n",
    "        255.0  # Normalize for matplotlib\n",
    "\n",
    "    # Apply the colored border\n",
    "    rgb_image[border] = border_color\n",
    "    rgb_image = (rgb_image * 255).astype(np.uint8)\n",
    "\n",
    "    if new_img is not None:\n",
    "        new_img = np.concatenate([rgb_image, new_img], axis=1)\n",
    "        new_img = new_img.astype(np.uint8)\n",
    "        if save_new_img_path is not None:\n",
    "            cv2.imwrite(f'{save_new_img_path}/{name_}', new_img)\n",
    "        if show_:\n",
    "            fig, ax = plt.subplots(figsize=(scale_*new_img.shape[1] / dpi, scale_*new_img.shape[0] / dpi))\n",
    "            ax.imshow(new_img, cmap='gray')\n",
    "            ax.axis('off')  # Turn off axis numbers\n",
    "    else:\n",
    "        if show_:\n",
    "            fig, ax = plt.subplots(figsize=(scale_*rgb_image.shape[1] / dpi, scale_*rgb_image.shape[0] / dpi))\n",
    "            ax.imshow(rgb_image, cmap='gray')\n",
    "            ax.axis('off')  # Turn off axis numbers\n",
    "        if save_overlay_img_path is not None:\n",
    "            cv2.imwrite(f'{save_overlay_img_path}/{name_}', rgb_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def overlay_mask_border_on_image_frm_img(\n",
    "        img:Union[OpenCvImage, Image.Image],\n",
    "        msk:Union[OpenCvImage, Image.Image],\n",
    "    ):\n",
    "    'Overlay mask border on image from image'\n",
    "    cv2_img = np.array(img) if isinstance(img, Image.Image) else img\n",
    "    cv2_img = cv2.cvtColor(cv2_img, cv2.COLOR_RGB2BGR)\n",
    "    # converting cv2 mask in case it is not \n",
    "    cv2_msk = np.array(msk) if isinstance(msk, Image.Image) else msk\n",
    "\n",
    "    msk_uint8 = (cv2_msk* 255).astype(np.uint8)\n",
    "    if len(msk_uint8.shape) == 3 and msk_uint8.shape[2] == 3:\n",
    "        msk_uint8 = cv2.cvtColor(msk_uint8, cv2.COLOR_RGB2GRAY)\n",
    "    cntrs, _, = cv2.findContours(msk_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(\n",
    "        cv2_img, \n",
    "        contours=cntrs, \n",
    "        contourIdx=-1, \n",
    "        color=(0, 255, 0), \n",
    "        thickness=2)\n",
    "    return cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#| export\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat_images\u001b[39m(\n\u001b[1;32m----> 3\u001b[0m         images:List[np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m      4\u001b[0m         rows:\u001b[38;5;28mint\u001b[39m,  \u001b[38;5;66;03m# number of rows\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         cols:\u001b[38;5;28mint\u001b[39m,  \u001b[38;5;66;03m# number of columns in combined images\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         number:\u001b[38;5;28mstr\u001b[39m \u001b[38;5;66;03m# a text which will be inserted in the combined image\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         ):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConcate images rows and cols and add a number to the image.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m     targe_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m images])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "def concat_images(\n",
    "        images:List[np.ndarray],\n",
    "        rows:int,  # number of rows\n",
    "        cols:int,  # number of columns in combined images\n",
    "        number:str, # a text which will be inserted in the combined image\n",
    "        border_size:int=10, # border size\n",
    "        border_color:Tuple[int, int, int]=(0, 0, 0) # border color\n",
    "        ):\n",
    "    'Concate images rows and cols and add a number to the image.'\n",
    "    targe_h = min([i.shape[0] for i in images])\n",
    "    target_w = min([i.shape[1] for i in images])\n",
    "    res_img = [cv2.resize(i, (target_w, targe_h)) for i in images]\n",
    "    res = []\n",
    "    for i in range(rows):\n",
    "        start_index = i * cols\n",
    "        end_index = start_index + cols\n",
    "        row_images = res_img[start_index:end_index]\n",
    "        row_images = [cv2.copyMakeBorder(img, 0, 0, 0, border_size, cv2.BORDER_CONSTANT, value=border_color) for img in row_images]\n",
    "        row_concat = cv2.hconcat(row_images)\n",
    "\n",
    "        res.append(row_concat)\n",
    "    new_img = cv2.vconcat(res)\n",
    "    position = (6, 25)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale=0.6\n",
    "    color = (255, 0, 0)\n",
    "    im_n = cv2.putText(new_img, f'{number}', position, font, font_scale, color, 1, cv2.LINE_AA)\n",
    "    return im_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_poster_from_path(\n",
    "                mask_path:str, \n",
    "                im_path:str, # name of the image fodler , e.g. 'images' or 'X'\n",
    "                show_:str, # whether to show image, mask, poster\n",
    "                text:str,# text in image\n",
    "                scale=1\n",
    "                ):\n",
    "    'Show only masked part of the image or full image with mask'\n",
    "    row_col_dict = {35:(5, 7), 21:(3,7),30:(5,6), 34:(2,17), 22:(2,11), 36:(6,6), 20:(5, 4), 33:(3,11) }\n",
    "\n",
    "    # getting mask and iamge name from mask\n",
    "    name = Path(mask_path).name\n",
    "    im_name = Path(im_path)/name\n",
    "\n",
    "    msk_ = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "    img_ = cv2.imread(str(im_name), cv2.IMREAD_GRAYSCALE)\n",
    "    img_ = cv2.cvtColor(img_, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # finding contours and base on that concat only contours\n",
    "    contrs, _ = cv2.findContours(msk_, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    row, col = row_col_dict.get(len(contrs), (1,len(contrs)))\n",
    "    cv2.drawContours(img_, contrs, -1, (0, 255, 0), 1)\n",
    "    images_list = []\n",
    "    for c in contrs:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        offset = 25\n",
    "        new_img = img_[y-offset:y+h+offset, x-offset:x+w+offset]\n",
    "        images_list.append(new_img)\n",
    "    img_new = concat_images(images_list, row, col, number=text)\n",
    "\n",
    "\n",
    "    # show imagess\n",
    "    if show_ == 'both':\n",
    "        print(img_new.shape, img_.shape)\n",
    "        img_ = cv2.resize(img_, (img_new.shape[1], img_new.shape[0]))\n",
    "        new_img = cv2.hconcat([img_, img_new])\n",
    "        res = new_img.shape\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(scale * res[1]/dpi, scale*res[0]/dpi))\n",
    "        ax.imshow(new_img)\n",
    "\n",
    "    elif show_ == 'image':\n",
    "        res = img_.shape\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(scale * res[1]/dpi, scale*res[0]/dpi))\n",
    "        ax.imshow(img_)\n",
    "    else:\n",
    "        res = img_new.shape\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(scale * res[1]/dpi, scale*res[0]/dpi))\n",
    "        ax.imshow(img_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export   \n",
    "def seamless_clone(\n",
    "        full_img:OpenCvImage, # gray scale image\n",
    "        replace_part:OpenCvImage, # gray scale image\n",
    "        center:Tuple[int, int],# center position in full_img #where the replace_part will be placed\n",
    "        clone_method:str='normal'\n",
    "    ):\n",
    "    'Replace some part of full_img with replace_part object'\n",
    "    full_img = cv2.cvtColor(full_img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    replace_part = cv2.cvtColor(replace_part, cv2.COLOR_GRAY2BGR)\n",
    "    mask = np.ones_like(replace_part)*255\n",
    "    if clone_method == 'mixed':\n",
    "        cloned_img = cv2.seamlessClone(\n",
    "            replace_part,\n",
    "            full_img,\n",
    "            mask,\n",
    "            center,\n",
    "            cv2.MIXED_CLONE)\n",
    "    else:\n",
    "        cloned_img = cv2.seamlessClone(\n",
    "            replace_part,\n",
    "            full_img,\n",
    "            mask,\n",
    "            center,\n",
    "            cv2.NORMAL_CLONE)\n",
    "    \n",
    "    return cv2.cvtColor(cloned_img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_template_part(\n",
    "    img:np.ndarray, #opencv image\n",
    "    tmp_img:np.ndarray # opencv image\n",
    "   ):\n",
    "   'Get bounding box coordinate from the image (x,y, w, h format)'\n",
    "\n",
    "   res = cv2.matchTemplate(\n",
    "                           img,\n",
    "                           tmp_img,\n",
    "                           cv2.TM_CCOEFF_NORMED\n",
    "   )\n",
    "\n",
    "   min_, max_, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "   top_left = max_loc\n",
    "   bottom_right = (top_left[0] + tmp_img.shape[1], top_left[1] + tmp_img.shape[0])\n",
    "\n",
    "   y = top_left[1]\n",
    "   h = bottom_right[1] - top_left[1]\n",
    "   x = top_left[0]\n",
    "   w = bottom_right[0] - top_left[0]\n",
    "   return x, y, w, h\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_image(\n",
    "        img:np.ndarray,\n",
    "        num_splits:int,\n",
    "        direction:str # horizontal or vertical\n",
    "        ):\n",
    "    'Split an image into different parts'\n",
    "\n",
    "    # Calculate the size of each split\n",
    "    if direction == 'horizontal':\n",
    "        split_size = img.shape[1] // num_splits\n",
    "    elif direction == 'vertical':\n",
    "        split_size = img.shape[0] // num_splits\n",
    "\n",
    "    # Split the image and store the parts in a list\n",
    "    parts = []\n",
    "    for i in range(num_splits):\n",
    "        if i == num_splits - 1:  # If this is the last split\n",
    "            if direction == 'horizontal':\n",
    "                part = img[:, i*split_size:]\n",
    "            elif direction == 'vertical':\n",
    "                part = img[i*split_size:, :]\n",
    "        else:\n",
    "            if direction == 'horizontal':\n",
    "                part = img[:, i*split_size:(i+1)*split_size]\n",
    "            elif direction == 'vertical':\n",
    "                part = img[i*split_size:(i+1)*split_size, :]\n",
    "        parts.append(part)\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_image_with_coordinates(\n",
    "        img: np.ndarray, \n",
    "        num_splits: int, \n",
    "        direction: str) -> list:\n",
    "    'Split an image with different parts in different coordinates'\n",
    "    indexed_parts_with_coords = []\n",
    "    split_size = img.shape[0] // num_splits if direction == 'vertical' else img.shape[1] // num_splits\n",
    "\n",
    "    for i in range(num_splits):\n",
    "        if direction == 'vertical':\n",
    "            start_y = i * split_size\n",
    "            end_y = img.shape[0] if i == num_splits - 1 else (i + 1) * split_size\n",
    "            part = img[start_y:end_y, :]\n",
    "            coordinates = (0, start_y, img.shape[1], end_y)\n",
    "        else:  # 'horizontal'\n",
    "            start_x = i * split_size\n",
    "            end_x = img.shape[1] if i == num_splits - 1 else (i + 1) * split_size\n",
    "            part = img[:, start_x:end_x]\n",
    "            coordinates = (start_x, 0, end_x, img.shape[0])\n",
    "        indexed_parts_with_coords.append((i, part, coordinates))\n",
    "\n",
    "    return indexed_parts_with_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_same_shape(\n",
    "        src_img:np.ndarray, # image which size needs to be replicated\n",
    "        dst_img:np.ndarray, # image which needs to resized\n",
    "    ):\n",
    "    'Create same shape of dst image like src_img'\n",
    "\n",
    "    h, w = src_img.shape[:2]\n",
    "    add_h, add_w = dst_img.shape[:2]\n",
    "\n",
    "    if h > add_h or w > add_w:\n",
    "        return cv2.resize(dst_img, (w, h))\n",
    "    else:\n",
    "        start_h, start_w = (add_h - h) // 2, (add_w - w) // 2\n",
    "        end_h, end_w = start_h + h, start_w + w\n",
    "        new = dst_img[start_h:end_h, start_w:end_w]\n",
    "        return new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_circle_from_single_pin(\n",
    "                            sn_pin_img:np.ndarray\n",
    "                              )->Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    'Get the circle from single pin image'\n",
    "\n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    pin_img = cv2.GaussianBlur(sn_pin_img, (5, 5), 0)\n",
    "\n",
    "\n",
    "    ## Apply Hough Circle Transform to detect circles\n",
    "    circles = cv2.HoughCircles(\n",
    "                               pin_img, \n",
    "                               cv2.HOUGH_GRADIENT, \n",
    "                               1, \n",
    "                               20,\n",
    "                               param1=50, \n",
    "                               param2=30, \n",
    "                               minRadius=0, \n",
    "                               maxRadius=0)\n",
    "    \n",
    "    # If circles are detected\n",
    "    if circles is not None:\n",
    "        # Get the coordinates and radius of the detected circle\n",
    "        circles = np.uint16(np.around(circles))\n",
    "\n",
    "        mask = np.zeros_like(pin_img)  # Mask image with same dimensions as original, initialized to black\n",
    "        rest = np.copy(pin_img)  # Copy of the original image to isolate the non-circular part\n",
    "        print(f' Number of circles found  = {len(circles[0])}')\n",
    "\n",
    "        for i in circles[0, :]:\n",
    "            # Draw the outer circle on the mask and fill it to create a solid circle\n",
    "            cv2.circle(mask, (i[0], i[1]), i[2], (255, 255, 255), thickness=-1)  # White circle on black background\n",
    "\n",
    "            # Cut the circular part from the rest image\n",
    "            cv2.circle(rest, (i[0], i[1]), i[2], (0, 0, 0), thickness=-1)  # Draw black circle on original\n",
    "\n",
    "        # Apply mask to the original image to extract only the circular part\n",
    "        segmented_circle = cv2.bitwise_and(pin_img, mask)\n",
    "        return segmented_circle, mask, rest\n",
    "    else:\n",
    "        print('No circles found')\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_contours_binary(\n",
    "    img:np.ndarray, # binary image \n",
    "    ):\n",
    "    'Return contours from the binary image'\n",
    "    cntrs, _= cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return cntrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def adjust_brightness(\n",
    "    img:np.ndarray, # image to adjust brightness\n",
    "    alpha:float, # alpha > 1 to brighten; alpha < 1 to darken\n",
    "    ):\n",
    "    'Adjust the brightness of the image'\n",
    "    adjusted = cv2.convertScaleAbs(\n",
    "        img, \n",
    "        alpha=alpha)  # alpha > 1 to brighten; alpha < 1 to darken\n",
    "    return adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ssim_(\n",
    "        img1:np.ndarray, \n",
    "        img2:np.ndarray,\n",
    "        win_size:int=5\n",
    "        ):\n",
    "    'Compare structural similarity between two images'\n",
    "    return ssim(img1, img2, win_size=win_size\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def orb_sim_(\n",
    "        img1:np.ndarray, \n",
    "        img2:np.ndarray):\n",
    "\n",
    "    'Compare ORB similarity between two images'\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "    kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    # finding how many regions are there\n",
    "\n",
    "    sim_reg = list(filter(lambda x: x.distance < 50, matches))\n",
    "    if len(matches) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(list(sim_reg))/len(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rot_based_on_ref_img(\n",
    "    ref_img:Union[np.ndarray, Image.Image], # Image will be used refeernece image\n",
    "    tst_img:Union[np.ndarray, Image.Image], # Image where searched for key points\n",
    "    show_m:bool=True # whether to show matching points of two images\n",
    "    )->np.ndarray:\n",
    "    'Rotate the tst_img based on ref image, find key points in ref image and then rotate tst_img'\n",
    "    if isinstance(ref_img, Image.Image):\n",
    "        ref_img = np.array(ref_img)\n",
    "    \n",
    "    if isinstance(tst_img, Image.Image):\n",
    "        tst_img = np.array(tst_img)\n",
    "    orb = cv2.ORB_create(50)\n",
    "    # find the keypoints and descriptors with orb\n",
    "    kp1, des1 = orb.detectAndCompute(tst_img, None)  #kp1 --> list of keypoints\n",
    "    kp2, des2 = orb.detectAndCompute(ref_img, None)\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    #Match descriptors.\n",
    "    matches = matcher.match(des1, des2, None) \n",
    "    #Creates a list of all matches, just like keypoints\n",
    "    img3 = cv2.drawMatches(tst_img,kp1, ref_img, kp2, matches[:10], None)\n",
    "    if show_m: show_(img3)\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)  \n",
    "    #Prints empty array of size equal to (matches, 2)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = kp1[match.queryIdx].pt    #gives index of the descriptor in the list of query descriptors\n",
    "        points2[i, :] = kp2[match.trainIdx].pt    #gives index of the descriptor in the list of train descriptors\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "    if len(tst_img.shape) > 2:\n",
    "        height, width, ch = ref_img.shape\n",
    "    else:\n",
    "        height, width = ref_img.shape\n",
    "    \n",
    "    im1Reg = cv2.warpPerspective(tst_img, h, (width, height)) \n",
    "    return im1Reg\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def frm_cntr_to_bbox(cntr):\n",
    "    x,y,w,h = cv2.boundingRect(cntr)\n",
    "    return x,y,w,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def foo(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export('00_core.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
