{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from fastcore.all import *\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Union, Dict, List, NewType\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.ndimage import (\n",
    "    label, binary_dilation, binary_erosion,label,\n",
    "    )\n",
    "from skimage.color import label2rgb\n",
    "from skimage.restoration import denoise_nl_means, estimate_sigma, denoise_tv_chambolle\n",
    "import yaml\n",
    "from skimage import img_as_float\n",
    "import matplotlib as mpl\n",
    "from PIL import ImageDraw\n",
    "import os\n",
    "import sys, traceback, gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def clean_ipython_hist():\n",
    "    # Code in this function mainly copied from IPython source\n",
    "    if not 'get_ipython' in globals(): return\n",
    "    ip = get_ipython()\n",
    "    user_ns = ip.user_ns\n",
    "    ip.displayhook.flush()\n",
    "    pc = ip.displayhook.prompt_count + 1\n",
    "    for n in range(1, pc): user_ns.pop('_i'+repr(n),None)\n",
    "    user_ns.update(dict(_i='',_ii='',_iii=''))\n",
    "    hm = ip.history_manager\n",
    "    hm.input_hist_parsed[:] = [''] * pc\n",
    "    hm.input_hist_raw[:] = [''] * pc\n",
    "    hm._i = hm._ii = hm._iii = hm._i00 =  ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def clean_tb():\n",
    "    # h/t Piotr Czapla\n",
    "    if hasattr(sys, 'last_traceback'):\n",
    "        traceback.clear_frames(sys.last_traceback)\n",
    "        delattr(sys, 'last_traceback')\n",
    "    if hasattr(sys, 'last_type'): delattr(sys, 'last_type')\n",
    "    if hasattr(sys, 'last_value'): delattr(sys, 'last_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clean_mem(torch_:bool=True):\n",
    "    clean_tb()\n",
    "    clean_ipython_hist()\n",
    "    gc.collect()\n",
    "    if torch_:\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "OpenCvImage = NewType('OpenCvImage', np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "get_name_ = np.vectorize(lambda x: x.name)\n",
    "dpi = mpl.rcParams['figure.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def filter_(self:Path, name_part:str):\n",
    "    'filter based on name_part in file' \n",
    "    return  L(filter(lambda x: name_part in x.name, self.ls()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_config(config_file:Path) -> Dict:\n",
    "    'read config file'\n",
    "    with open (f'{config_file}', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def label_mask(\n",
    "        mask:np.ndarray\n",
    "        )->np.ndarray:\n",
    "    \"Label connected components in a binary mask\"\n",
    "    try: \n",
    "        labels, num_labels = label(mask)\n",
    "        return labels, num_labels\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_rotation_angle(\n",
    "    ref_img:np.ndarray, \n",
    "    tst_img:np.ndarray,\n",
    "    method:str='sift' # sift or orb\n",
    "    )->float:\n",
    "    'Find rotation angle between ref image and test image'\n",
    "    if method == 'sift':\n",
    "        # Initialize SIFT detector\n",
    "        sift = cv2.SIFT_create()\n",
    "\n",
    "        # Find keypoints and descriptors\n",
    "        kp1, des1 = sift.detectAndCompute(ref_img, None)\n",
    "        kp2, des2 = sift.detectAndCompute(tst_img, None)\n",
    "\n",
    "        # FLANN parameters\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "\n",
    "        # FLANN-based matcher\n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "        # Apply ratio test\n",
    "        good_matches = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.7 * n.distance:\n",
    "                good_matches.append(m)\n",
    "\n",
    "        # Extract matched keypoints\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    else:\n",
    "        method = 'orb'\n",
    "        detector = cv2.ORB_create()\n",
    "        kp1, des1 = detector.detectAndCompute(ref_img, None)\n",
    "        kp2, des2 = detector.detectAndCompute(tst_img, None)\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = bf.match(des1, des2)\n",
    "        matches = sorted(matches, key = lambda x:x.distance)\n",
    "        good_matches = matches[:10]\n",
    "\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 2)\n",
    "\n",
    "\n",
    "\n",
    "    # Find homography\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Extract rotation angle from homography matrix\n",
    "    angle = np.arctan2(M[1, 0], M[0, 0]) * 180 / np.pi\n",
    "\n",
    "    return angle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rotate_image(\n",
    "    image:Union[OpenCvImage,Image.Image], # image \n",
    "    angle:int, # angle to rotate\n",
    "    interpolation_method=cv2.INTER_CUBIC\n",
    "    )-> OpenCvImage: # return only rotated image\n",
    "    ' Rotate image by angle'\n",
    "     # if Pil image\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rot_image = cv2.warpAffine(\n",
    "        image, \n",
    "        M, \n",
    "        (w, h), \n",
    "        flags=interpolation_method,\n",
    "        borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rot_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_labeled_mask(\n",
    "        msk_path):\n",
    "    \n",
    "    msk_img = cv2.imread(str(msk_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    labels, num_labels = label_mask(msk_img)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.imshow(labels)\n",
    "    # Annotate each object with its label number\n",
    "    ax.set_facecolor('black')\n",
    "    for i in range(1, num_labels + 1):\n",
    "        y, x = np.where(labels == i)\n",
    "        centroid = (x.mean(), y.mean())\n",
    "        ax.text(centroid[0], centroid[1], str(i), color='white', ha='center', va='center', fontsize=12)\n",
    "    plt.axis('off')\n",
    "    return labels, num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_new_mask(\n",
    "                     new_lb, # labeled masks\n",
    "                     new_mask_path, # path to save the new mask\n",
    "                     fn # name of the image\n",
    "                     ):\n",
    "    'Write mask to new_mask_path'\n",
    "    new_lbl = new_lb.astype(np.uint8)\n",
    "    new_lbl = np.where(new_lbl > .9, 255, 0)\n",
    "    cv2.imwrite(f'{new_mask_path}/{fn}', new_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_object_from_mask(\n",
    "                           mask:np.ndarray,\n",
    "                           object_id_list:List[int]\n",
    "                           ):\n",
    "    \"\"\"Remove object from mask.\"\"\"\n",
    "\n",
    "    for i in object_id_list:\n",
    "        mask[mask == i] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_img(\n",
    "    im_path:Union[str, Path],\n",
    "    cv:bool=True,\n",
    "    gray:bool=True\n",
    "    ):\n",
    "    'Read image from name could be open cv or pil image'\n",
    "    if cv:\n",
    "        if gray:\n",
    "            return cv2.imread(f'{im_path}', cv2.IMREAD_GRAYSCALE)\n",
    "        else:\n",
    "            return cv2.cvtColor(cv2.imread(f'{im_path}'), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    else:\n",
    "        img = Image.open(im_path)\n",
    "        if gray:\n",
    "            return img.convert('L')\n",
    "        else:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def blur_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        ks:int=3, # kernel size\n",
    "        ) -> np.ndarray:\n",
    "    'Blur image'\n",
    "    return cv2.GaussianBlur(img, (ks, ks), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def median_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        ks:int=3, # kernel size\n",
    "        ) -> np.ndarray:\n",
    "    'Median image'\n",
    "    return cv2.medianBlur(img, ks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def canny_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        th1:int=50, #threshold1\n",
    "        th2:int=150, #threshold2\n",
    "        ) -> np.ndarray:\n",
    "    'Canny image'\n",
    "    return cv2.Canny(img, th1, th2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sharpen_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        ) -> np.ndarray:\n",
    "    'Sharpen image'\n",
    "    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def bilateral_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        d:int=9, # diameter of each pixel neighborhood\n",
    "        sc:int=75, # sigma color\n",
    "        ss:int=75, # sigma space\n",
    "        ) -> np.ndarray:\n",
    "    'Bilateral image'\n",
    "    return cv2.bilateralFilter(img, d, sc, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def nlm_filter_(\n",
    "    img:np.ndarray,  #image\n",
    "    h:float=1.15,\n",
    "    fast_mode:bool=True,\n",
    "    patch_size:int=5,\n",
    "    patch_distance:int=3\n",
    "\n",
    "  ):\n",
    "  'Apply Non local means denoising filter to image'\n",
    "  img = img.astype(np.float)\n",
    "  sigma_est = np.mean(\n",
    "    estimate_sigma(img) \n",
    "    )\n",
    "  denoise_img = denoise_nl_means(\n",
    "    img, \n",
    "    h=h* sigma_est, \n",
    "    fast_mode=fast_mode,\n",
    "    patch_size=patch_size,\n",
    "    patch_distance=patch_distance,\n",
    "    )\n",
    "  return denoise_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tv_filter_(\n",
    "    img:np.ndarray,  #image\n",
    "    weight:float=0.1, # greater the weight, more denoising, but with less details\n",
    "    max_num_iter:int=200,\n",
    "\n",
    "    ):\n",
    "    'Apply total variation denoising filter to image'\n",
    "    #img = img.astype(np.float)\n",
    "    #d_img = denoise_tv_chambolle(\n",
    "        #img, \n",
    "        #weight=0.1, \n",
    "        #eps=0.0002,\n",
    "        #max_num_iter=max_num_iter,\n",
    "        #)\n",
    "    dst = cv2.fastNlMeansDenoising(np.clip(img,0,255), None, 4, 7, 35)\n",
    "    return dst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev\n",
    "nbdev.nbdev_export('00_core.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_img_(\n",
    "        img:np.ndarray,  #image\n",
    "        filters_:List[str] # opencv filters\n",
    "        ) -> np.ndarray:\n",
    "\n",
    "    'Apply filter on image'\n",
    "\n",
    "    ops = {\n",
    "        'blur': blur_img_,\n",
    "        'med': median_img_,\n",
    "        'canny': canny_img_,\n",
    "        'sharpen': sharpen_img_,\n",
    "        'nlm': nlm_filter_,\n",
    "        'tv': tv_filter_,\n",
    "        'bilateral': bilateral_img_,\n",
    "    }\n",
    "    for fl in filters_:\n",
    "        if fl in ops:\n",
    "            img = ops[fl](img)\n",
    "        else:\n",
    "            print(f'Filter {fl} not found')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_text_to_image(\n",
    "                        img: np.ndarray, \n",
    "                        text: str, \n",
    "                        position: Tuple[int, int]=None,\n",
    "                        font=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        font_scale: float=1, \n",
    "                        color: Tuple[int, int, int]=(255, 255, 255), \n",
    "                        thickness: int=2):\n",
    "    \"\"\"\n",
    "    Add text to an image.\n",
    "\n",
    "    Parameters:\n",
    "    img (np.ndarray): The image to add text to.\n",
    "    text (str): The text to add.\n",
    "    position (Tuple[int, int]): The position where the text should be added.\n",
    "    font: The font to use. Default is cv2.FONT_HERSHEY_SIMPLEX.\n",
    "    font_scale (float): The font scale. Default is 1.\n",
    "    color (Tuple[int, int, int]): The color of the text in BGR format. Default is white.\n",
    "    thickness (int): The thickness of the text. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The image with the text added.\n",
    "    \"\"\"\n",
    "    if position is None:\n",
    "        # Set the position to a small offset from the top left corner\n",
    "        position = (10, 30)  # 10 pixels from the left, 30 pixels from the top\n",
    "    img_with_text = cv2.putText(img.copy(), text, position, font, font_scale, color, thickness)\n",
    "    return img_with_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_image_posters_with_analysis(input_folder, output_folder, rows, cols):\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Get list of image files\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "    \n",
    "    # Analyze images and create dataframe\n",
    "    data = []\n",
    "    for idx, img_file in enumerate(image_files):\n",
    "        img_path = os.path.join(input_folder, img_file)\n",
    "        data.append({\n",
    "            'index': idx,\n",
    "            'filename': img_file,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.set_index('index', inplace=True)\n",
    "    \n",
    "    # Save dataframe\n",
    "    df.to_csv(os.path.join(output_folder, 'image_analysis.csv'))\n",
    "    \n",
    "    # Calculate the number of images per poster\n",
    "    images_per_poster = rows * cols\n",
    "    \n",
    "    # Calculate the number of posters needed\n",
    "    num_posters = (len(image_files) + images_per_poster - 1) // images_per_poster\n",
    "    \n",
    "    for poster_num in range(num_posters):\n",
    "        # Select images for this poster\n",
    "        start_idx = poster_num * images_per_poster\n",
    "        end_idx = min(start_idx + images_per_poster, len(image_files))\n",
    "        poster_images = image_files[start_idx:end_idx]\n",
    "        \n",
    "        # Open first image to get dimensions\n",
    "        with Image.open(os.path.join(input_folder, poster_images[0])) as img:\n",
    "            img_width, img_height = img.size\n",
    "        \n",
    "        # Create a new blank image for the poster\n",
    "        poster_width = img_width * cols\n",
    "        poster_height = img_height * rows\n",
    "        poster = Image.new('RGB', (poster_width, poster_height))\n",
    "        \n",
    "        # Paste images onto the poster\n",
    "        for idx, img_file in enumerate(poster_images):\n",
    "            row = idx // cols\n",
    "            col = idx % cols\n",
    "            with Image.open(os.path.join(input_folder, img_file)) as img:\n",
    "                # Add index text to the image with green color and larger font\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                #font = ImageFont.truetype('/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', 24)  # Use a larger font size\n",
    "                text = str(start_idx + idx)\n",
    "                draw.text((10, 10), text, fill=\"green\")  # Use green color for text\n",
    "                \n",
    "                poster.paste(img, (col * img_width, row * img_height))\n",
    "        \n",
    "        # Save the poster\n",
    "        poster.save(os.path.join(output_folder, f'poster_{poster_num + 1}.jpg'))\n",
    "        print(f'Created poster_{poster_num + 1}.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def denoise_img(\n",
    "    img:np.ndarray,  #image\n",
    "    filters_:List[str], # opencv filters\n",
    "    h:float=1.15,\n",
    "    fast_mode:bool=True,\n",
    "    patch_size:int=5,\n",
    "    patch_distance:int=3,\n",
    "    multichannel:bool=False,\n",
    "    weight:float=0.1, # greater the weight, more denoising, but with less details\n",
    "    n_iter_max:int=200,\n",
    "    ):\n",
    "    'Apply filter on image'\n",
    "\n",
    "    ops = {\n",
    "        'blur': blur_img_,\n",
    "        'med': median_img_,\n",
    "        'canny': canny_img_,\n",
    "        'sharpen': sharpen_img_,\n",
    "        'nlm': nlm_filter_,\n",
    "        'tv': tv_filter_,\n",
    "        'bilateral': bilateral_img_,\n",
    "    }\n",
    "    images_= []\n",
    "    for fl in filters_:\n",
    "        if fl in ops:\n",
    "            img_ = ops[fl](img)\n",
    "            image_ = add_text_to_image(img_, fl)\n",
    "            images_.append(image_)\n",
    "\n",
    "        else:\n",
    "            print(f'Filter {fl} not found')\n",
    "    return show_(np.concatenate(images_, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_(\n",
    "    im_path:Union[str, np.ndarray],\n",
    "\tcmap:str='gray'\n",
    "    ):\n",
    "    'Showing an image could be image or str, '\n",
    "\n",
    "    if isinstance(im_path,str) or isinstance(im_path, Path):\n",
    "        im = Image.open(im_path)\n",
    "        fig, ax = plt.subplots(figsize= (im.size[0]/dpi, im.size[1]/dpi))\n",
    "        ax.imshow(im, cmap=cmap)\n",
    "        ax.axis('off')\n",
    "    elif isinstance(im_path, list):\n",
    "            for i in im_path:\n",
    "                if isinstance(i, str) or isinstance(i, Path):\n",
    "                    im = Image.open(i)\n",
    "                    h, w = im.size\n",
    "                    fig, ax = plt.subplots(figsize= (w/dpi, h/dpi))\n",
    "                    ax.imshow(i, cmap=cmap)\n",
    "                    ax.axis('off')\n",
    "                elif isinstance(i, np.ndarray):\n",
    "                    h, w = i.shape[0], i.shape[1]\n",
    "                    fig, ax = plt.subplots(figsize= (w/dpi, h/dpi))\n",
    "                    ax.imshow(i, cmap=cmap)\n",
    "                    ax.axis('off')\n",
    "                    \n",
    "                    \n",
    "    else: \n",
    "        h, w = im_path.shape[0], im_path.shape[1]\n",
    "        fig, ax = plt.subplots(figsize= (w/dpi, h/dpi))\n",
    "        im = im_path\n",
    "        ax.imshow(im,cmap=cmap)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#def normalize(\n",
    "              #image:Union[np.ndarray, tf.Tensor], \n",
    "              #min=0):\n",
    "    #def _normalize(im):\n",
    "        #img = tf.cast(im, tf.float32)\n",
    "        #return img / 255.0\n",
    "\n",
    "    #if min == 0:\n",
    "        #return _normalize(image)\n",
    "    #else:\n",
    "        #return (_normalize(image) * 2.0) -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def center_crop(\n",
    "        image:Image,# not open cv but PIL Image \n",
    "        desired_width:int=1632,\n",
    "        desired_height:int=1152,\n",
    "        height_offset:int=-50,\n",
    "        width_offset:int=-70,\n",
    "        cv:bool=True\n",
    "        ):\n",
    "    # Get the current size of the image\n",
    "    width, height = image.size\n",
    "\n",
    "    # Calculate the coordinates of the center of the image\n",
    "    center_x = (width // 2 + (width_offset))\n",
    "    center_y = (height // 2 + (height_offset))\n",
    "\n",
    "    # Calculate the coordinates of the top-left corner of the crop\n",
    "    left = center_x - desired_width // 2\n",
    "    top = center_y - desired_height // 2\n",
    "\n",
    "    # Calculate the coordinates of the bottom-right corner of the crop\n",
    "    right = center_x + desired_width // 2\n",
    "    bottom = center_y + desired_height // 2\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "    #print(f'cropped_image size = {cropped_image.size}')\n",
    "    if cv:\n",
    "        return np.array(cropped_image)\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def overlay_mask(\n",
    "        im_path:Union[str,Path],\n",
    "        msk_path:Union[str,Path], \n",
    "        overlay_clr:Tuple[int, int, int]=(0, 1, 0), # color\n",
    "        scale:int=1, # to scale the image \n",
    "        alpha:float=0.5, # visibility\n",
    "        ):\n",
    "    'Creaete a overlay image from image and mask'\n",
    "    # Read the grayscale image\n",
    "    gray_img = cv2.imread(f'{im_path}', cv2.IMREAD_GRAYSCALE)\n",
    "    if gray_img is None:\n",
    "        raise ValueError(\"Could not read the grayscale image\")\n",
    "\n",
    "    # Read the mask image\n",
    "    mask_img = cv2.imread(f'{msk_path}', cv2.IMREAD_GRAYSCALE)\n",
    "    mask_img = mask_img.astype(bool)\n",
    "    if mask_img is None:\n",
    "        raise ValueError(\"Could not read the mask image\")\n",
    "\n",
    "    # Check if dimensions of both images are the same\n",
    "    if gray_img.shape != mask_img.shape:\n",
    "        raise ValueError(\"Dimensions of grayscale image and mask do not match\")\n",
    "\n",
    "    # Convert image to 3 channels\n",
    "    rgb_img = np.stack([gray_img]*3, axis=-1)/255\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(rgb_img)\n",
    "\n",
    "    clrd_overlay = np.zeros_like(rgb_img)\n",
    "    clrd_overlay[mask_img]=overlay_clr\n",
    "    ax.imshow(clrd_overlay, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def overlay_mask_border_on_image(\n",
    "        im_path: Union[Path, str],\n",
    "        msk_path: Union[Path, str],\n",
    "        save_new_img_path:Union[Path,str]=None,\n",
    "        save_overlay_img_path:Union[Path,str]=None,\n",
    "        new_img:Union[List, np.ndarray, None] = None,\n",
    "        scale_:int=1,\n",
    "        border_color: Tuple[int, int, int] = (0, 1, 0),\n",
    "        border_width: int = 1,\n",
    "        show_:bool=False):\n",
    "    \"\"\"\n",
    "    Overlays the border of a binary mask on a grayscale image and displays the result using matplotlib.\n",
    "\n",
    "    Args:\n",
    "    image (numpy.ndarray): Grayscale image.\n",
    "    mask (numpy.ndarray): Binary mask of the same size as the image.\n",
    "    border_color (tuple): RGB color for the mask border in the range [0, 1].\n",
    "    border_width (int): Width of the border.\n",
    "\n",
    "    Returns:\n",
    "    None: The function displays a plot.\n",
    "    \"\"\"\n",
    "    name_ = Path(im_path).name\n",
    "    gray_img = cv2.imread(f'{im_path}', cv2.IMREAD_GRAYSCALE)\n",
    "    if gray_img is None:\n",
    "        raise ValueError(\"Could not read the grayscale image\")\n",
    "\n",
    "    # Read the mask image\n",
    "    mask_img = cv2.imread(f'{msk_path}', cv2.IMREAD_GRAYSCALE)\n",
    "    mask_img = mask_img.astype(bool)\n",
    "    if mask_img is None:\n",
    "        raise ValueError(\"Could not read the mask image\")\n",
    "\n",
    "    # Check if dimensions of both images are the same\n",
    "    if gray_img.shape != mask_img.shape:\n",
    "        raise ValueError(\"Dimensions of grayscale image and mask do not match\")\n",
    "    # Ensure the mask is boolean\n",
    "\n",
    "    # Find the borders of the mask\n",
    "    dilated_mask = binary_dilation(mask_img, iterations=border_width)\n",
    "    eroded_mask = binary_erosion(mask_img, iterations=border_width)\n",
    "    border = dilated_mask & ~eroded_mask\n",
    "\n",
    "    # Convert grayscale image to RGB\n",
    "    rgb_image = np.stack([gray_img]*3, axis=-1) / \\\n",
    "        255.0  # Normalize for matplotlib\n",
    "\n",
    "    # Apply the colored border\n",
    "    rgb_image[border] = border_color\n",
    "    rgb_image = (rgb_image * 255).astype(np.uint8)\n",
    "\n",
    "    if new_img is not None:\n",
    "        new_img = np.concatenate([rgb_image, new_img], axis=1)\n",
    "        new_img = new_img.astype(np.uint8)\n",
    "        if save_new_img_path is not None:\n",
    "            cv2.imwrite(f'{save_new_img_path}/{name_}', new_img)\n",
    "        if show_:\n",
    "            fig, ax = plt.subplots(figsize=(scale_*new_img.shape[1] / dpi, scale_*new_img.shape[0] / dpi))\n",
    "            ax.imshow(new_img, cmap='gray')\n",
    "            ax.axis('off')  # Turn off axis numbers\n",
    "    else:\n",
    "        if show_:\n",
    "            fig, ax = plt.subplots(figsize=(scale_*rgb_image.shape[1] / dpi, scale_*rgb_image.shape[0] / dpi))\n",
    "            ax.imshow(rgb_image, cmap='gray')\n",
    "            ax.axis('off')  # Turn off axis numbers\n",
    "        if save_overlay_img_path is not None:\n",
    "            cv2.imwrite(f'{save_overlay_img_path}/{name_}', rgb_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def overlay_mask_border_on_image_frm_img(\n",
    "        img:Union[OpenCvImage, Image.Image],\n",
    "        msk:Union[OpenCvImage, Image.Image],\n",
    "    ):\n",
    "    'Overlay mask border on image from image'\n",
    "    cv2_img = np.array(img) if isinstance(img, Image.Image) else img\n",
    "    cv2_img = cv2.cvtColor(cv2_img, cv2.COLOR_RGB2BGR)\n",
    "    # converting cv2 mask in case it is not \n",
    "    cv2_msk = np.array(msk) if isinstance(msk, Image.Image) else msk\n",
    "\n",
    "    msk_uint8 = (cv2_msk* 255).astype(np.uint8)\n",
    "    if len(msk_uint8.shape) == 3 and msk_uint8.shape[2] == 3:\n",
    "        msk_uint8 = cv2.cvtColor(msk_uint8, cv2.COLOR_RGB2GRAY)\n",
    "    cntrs, _, = cv2.findContours(msk_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(\n",
    "        cv2_img, \n",
    "        contours=cntrs, \n",
    "        contourIdx=-1, \n",
    "        color=(0, 255, 0), \n",
    "        thickness=2)\n",
    "    return cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#| export\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat_images\u001b[39m(\n\u001b[1;32m----> 3\u001b[0m         images:List[np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m      4\u001b[0m         rows:\u001b[38;5;28mint\u001b[39m,  \u001b[38;5;66;03m# number of rows\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         cols:\u001b[38;5;28mint\u001b[39m,  \u001b[38;5;66;03m# number of columns in combined images\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         number:\u001b[38;5;28mstr\u001b[39m \u001b[38;5;66;03m# a text which will be inserted in the combined image\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         ):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConcate images rows and cols and add a number to the image.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m     targe_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m images])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "def concat_images(\n",
    "        images:List[np.ndarray],\n",
    "        rows:int,  # number of rows\n",
    "        cols:int,  # number of columns in combined images\n",
    "        number:str, # a text which will be inserted in the combined image\n",
    "        border_size:int=10, # border size\n",
    "        border_color:Tuple[int, int, int]=(0, 0, 0) # border color\n",
    "        ):\n",
    "    'Concate images rows and cols and add a number to the image.'\n",
    "    targe_h = min([i.shape[0] for i in images])\n",
    "    target_w = min([i.shape[1] for i in images])\n",
    "    res_img = [cv2.resize(i, (target_w, targe_h)) for i in images]\n",
    "    res = []\n",
    "    for i in range(rows):\n",
    "        start_index = i * cols\n",
    "        end_index = start_index + cols\n",
    "        row_images = res_img[start_index:end_index]\n",
    "        row_images = [cv2.copyMakeBorder(img, 0, 0, 0, border_size, cv2.BORDER_CONSTANT, value=border_color) for img in row_images]\n",
    "        row_concat = cv2.hconcat(row_images)\n",
    "\n",
    "        res.append(row_concat)\n",
    "    new_img = cv2.vconcat(res)\n",
    "    position = (6, 25)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale=0.6\n",
    "    color = (255, 0, 0)\n",
    "    im_n = cv2.putText(new_img, f'{number}', position, font, font_scale, color, 1, cv2.LINE_AA)\n",
    "    return im_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_poster_from_path(\n",
    "                mask_path:str, \n",
    "                im_path:str, # name of the image fodler , e.g. 'images' or 'X'\n",
    "                show_:str, # whether to show image, mask, poster\n",
    "                text:str,# text in image\n",
    "                scale=1\n",
    "                ):\n",
    "    'Show only masked part of the image or full image with mask'\n",
    "    row_col_dict = {35:(5, 7), 21:(3,7),30:(5,6), 34:(2,17), 22:(2,11), 36:(6,6), 20:(5, 4), 33:(3,11) }\n",
    "\n",
    "    # getting mask and iamge name from mask\n",
    "    name = Path(mask_path).name\n",
    "    im_name = Path(im_path)/name\n",
    "\n",
    "    msk_ = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "    img_ = cv2.imread(str(im_name), cv2.IMREAD_GRAYSCALE)\n",
    "    img_ = cv2.cvtColor(img_, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # finding contours and base on that concat only contours\n",
    "    contrs, _ = cv2.findContours(msk_, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    row, col = row_col_dict.get(len(contrs), (1,len(contrs)))\n",
    "    cv2.drawContours(img_, contrs, -1, (0, 255, 0), 1)\n",
    "    images_list = []\n",
    "    for c in contrs:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        offset = 25\n",
    "        new_img = img_[y-offset:y+h+offset, x-offset:x+w+offset]\n",
    "        images_list.append(new_img)\n",
    "    img_new = concat_images(images_list, row, col, number=text)\n",
    "\n",
    "\n",
    "    # show imagess\n",
    "    if show_ == 'both':\n",
    "        print(img_new.shape, img_.shape)\n",
    "        img_ = cv2.resize(img_, (img_new.shape[1], img_new.shape[0]))\n",
    "        new_img = cv2.hconcat([img_, img_new])\n",
    "        res = new_img.shape\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(scale * res[1]/dpi, scale*res[0]/dpi))\n",
    "        ax.imshow(new_img)\n",
    "\n",
    "    elif show_ == 'image':\n",
    "        res = img_.shape\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(scale * res[1]/dpi, scale*res[0]/dpi))\n",
    "        ax.imshow(img_)\n",
    "    else:\n",
    "        res = img_new.shape\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(scale * res[1]/dpi, scale*res[0]/dpi))\n",
    "        ax.imshow(img_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export   \n",
    "def seamless_clone(\n",
    "        full_img:OpenCvImage, # gray scale image\n",
    "        replace_part:OpenCvImage, # gray scale image\n",
    "        center:Tuple[int, int],# center position in full_img #where the replace_part will be placed\n",
    "        clone_method:str='normal'\n",
    "    ):\n",
    "    'Replace some part of full_img with replace_part object'\n",
    "    full_img = cv2.cvtColor(full_img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    replace_part = cv2.cvtColor(replace_part, cv2.COLOR_GRAY2BGR)\n",
    "    mask = np.ones_like(replace_part)*255\n",
    "    if clone_method == 'mixed':\n",
    "        cloned_img = cv2.seamlessClone(\n",
    "            replace_part,\n",
    "            full_img,\n",
    "            mask,\n",
    "            center,\n",
    "            cv2.MIXED_CLONE)\n",
    "    else:\n",
    "        cloned_img = cv2.seamlessClone(\n",
    "            replace_part,\n",
    "            full_img,\n",
    "            mask,\n",
    "            center,\n",
    "            cv2.NORMAL_CLONE)\n",
    "    \n",
    "    return cv2.cvtColor(cloned_img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_template_part(\n",
    "    img:np.ndarray, #opencv image\n",
    "    tmp_img:np.ndarray # opencv image\n",
    "   ):\n",
    "   'Get bounding box coordinate from the image (x,y, w, h format)'\n",
    "\n",
    "   res = cv2.matchTemplate(\n",
    "                           img,\n",
    "                           tmp_img,\n",
    "                           cv2.TM_CCOEFF_NORMED\n",
    "   )\n",
    "\n",
    "   min_, max_, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "   top_left = max_loc\n",
    "   bottom_right = (top_left[0] + tmp_img.shape[1], top_left[1] + tmp_img.shape[0])\n",
    "\n",
    "   y = top_left[1]\n",
    "   h = bottom_right[1] - top_left[1]\n",
    "   x = top_left[0]\n",
    "   w = bottom_right[0] - top_left[0]\n",
    "   return x, y, w, h\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_images_grid(images, rows, cols):\n",
    "    \"\"\"\n",
    "    Plot a list of images in a grid without white space between rows.\n",
    "\n",
    "    Parameters:\n",
    "    images (list of np.ndarray): List of images to plot.\n",
    "    rows (int): Number of rows in the grid.\n",
    "    cols (int): Number of columns in the grid.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    fig.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([axes])  # Convert to 1D array for consistency\n",
    "\n",
    "    for ax, img in zip(axes.flatten(), images):\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0, hspace=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_template_part_img(\n",
    "        im:np.ndarray,  # image\n",
    "        tm_im:np.ndarray,  # template image\n",
    "        show_im:bool = False\n",
    "        ):\n",
    "\t'Get temlate part from whole image'\n",
    "\tx, y, w, h = get_template_part(im, tm_im)\n",
    "\tim_crop = im[y:y+h, x:x+w]\n",
    "\tif show_im:\n",
    "\t\tshow_(im_crop)\n",
    "\treturn im_crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_image(\n",
    "        img:np.ndarray,\n",
    "        num_splits:int,\n",
    "        direction:str # horizontal or vertical\n",
    "        ):\n",
    "    'Split an image into different parts'\n",
    "\n",
    "    # Calculate the size of each split\n",
    "    if direction == 'horizontal':\n",
    "        split_size = img.shape[1] // num_splits\n",
    "    elif direction == 'vertical':\n",
    "        split_size = img.shape[0] // num_splits\n",
    "\n",
    "    # Split the image and store the parts in a list\n",
    "    parts = []\n",
    "    for i in range(num_splits):\n",
    "        if i == num_splits - 1:  # If this is the last split\n",
    "            if direction == 'horizontal':\n",
    "                part = img[:, i*split_size:]\n",
    "            elif direction == 'vertical':\n",
    "                part = img[i*split_size:, :]\n",
    "        else:\n",
    "            if direction == 'horizontal':\n",
    "                part = img[:, i*split_size:(i+1)*split_size]\n",
    "            elif direction == 'vertical':\n",
    "                part = img[i*split_size:(i+1)*split_size, :]\n",
    "        parts.append(part)\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_image_with_coordinates(\n",
    "        img: np.ndarray, \n",
    "        num_splits: int, \n",
    "        direction: str) -> list:\n",
    "    'Split an image with different parts in different coordinates'\n",
    "    indexed_parts_with_coords = []\n",
    "    split_size = img.shape[0] // num_splits if direction == 'vertical' else img.shape[1] // num_splits\n",
    "\n",
    "    for i in range(num_splits):\n",
    "        if direction == 'vertical':\n",
    "            start_y = i * split_size\n",
    "            end_y = img.shape[0] if i == num_splits - 1 else (i + 1) * split_size\n",
    "            part = img[start_y:end_y, :]\n",
    "            coordinates = (0, start_y, img.shape[1], end_y)\n",
    "        else:  # 'horizontal'\n",
    "            start_x = i * split_size\n",
    "            end_x = img.shape[1] if i == num_splits - 1 else (i + 1) * split_size\n",
    "            part = img[:, start_x:end_x]\n",
    "            coordinates = (start_x, 0, end_x, img.shape[0])\n",
    "        indexed_parts_with_coords.append((i, part, coordinates))\n",
    "\n",
    "    return indexed_parts_with_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_same_shape(\n",
    "        src_img:np.ndarray, # image which size needs to be replicated\n",
    "        dst_img:np.ndarray, # image which needs to resized\n",
    "    ):\n",
    "    'Create same shape of dst image like src_img'\n",
    "\n",
    "    h, w = src_img.shape[:2]\n",
    "    add_h, add_w = dst_img.shape[:2]\n",
    "\n",
    "    if h > add_h or w > add_w:\n",
    "        return cv2.resize(dst_img, (w, h))\n",
    "    else:\n",
    "        start_h, start_w = (add_h - h) // 2, (add_w - w) // 2\n",
    "        end_h, end_w = start_h + h, start_w + w\n",
    "        new = dst_img[start_h:end_h, start_w:end_w]\n",
    "        return new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_circle_from_single_pin(\n",
    "                            sn_pin_img:np.ndarray\n",
    "                              )->Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    'Get the circle from single pin image'\n",
    "\n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    pin_img = cv2.GaussianBlur(sn_pin_img, (5, 5), 0)\n",
    "\n",
    "\n",
    "    ## Apply Hough Circle Transform to detect circles\n",
    "    circles = cv2.HoughCircles(\n",
    "                               pin_img, \n",
    "                               cv2.HOUGH_GRADIENT, \n",
    "                               1, \n",
    "                               20,\n",
    "                               param1=50, \n",
    "                               param2=30, \n",
    "                               minRadius=0, \n",
    "                               maxRadius=0)\n",
    "    \n",
    "    # If circles are detected\n",
    "    if circles is not None:\n",
    "        # Get the coordinates and radius of the detected circle\n",
    "        circles = np.uint16(np.around(circles))\n",
    "\n",
    "        mask = np.zeros_like(pin_img)  # Mask image with same dimensions as original, initialized to black\n",
    "        rest = np.copy(pin_img)  # Copy of the original image to isolate the non-circular part\n",
    "        print(f' Number of circles found  = {len(circles[0])}')\n",
    "\n",
    "        for i in circles[0, :]:\n",
    "            # Draw the outer circle on the mask and fill it to create a solid circle\n",
    "            cv2.circle(mask, (i[0], i[1]), i[2], (255, 255, 255), thickness=-1)  # White circle on black background\n",
    "\n",
    "            # Cut the circular part from the rest image\n",
    "            cv2.circle(rest, (i[0], i[1]), i[2], (0, 0, 0), thickness=-1)  # Draw black circle on original\n",
    "\n",
    "        # Apply mask to the original image to extract only the circular part\n",
    "        segmented_circle = cv2.bitwise_and(pin_img, mask)\n",
    "        return segmented_circle, mask, rest\n",
    "    else:\n",
    "        print('No circles found')\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_contours_binary(\n",
    "    img:np.ndarray, # binary image \n",
    "    ):\n",
    "    'Return contours from the binary image'\n",
    "    cntrs, _= cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return cntrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def adjust_brightness(\n",
    "    img:np.ndarray, # image to adjust brightness\n",
    "    alpha:float, # alpha > 1 to brighten; alpha < 1 to darken\n",
    "    ):\n",
    "    'Adjust the brightness of the image'\n",
    "    adjusted = cv2.convertScaleAbs(\n",
    "        img, \n",
    "        alpha=alpha)  # alpha > 1 to brighten; alpha < 1 to darken\n",
    "    return adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ssim_(\n",
    "        img1:np.ndarray, \n",
    "        img2:np.ndarray,\n",
    "        win_size:int=5\n",
    "        ):\n",
    "    'Compare structural similarity between two images'\n",
    "    return ssim(img1, img2, win_size=win_size\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def orb_sim_(\n",
    "        img1:np.ndarray, \n",
    "        img2:np.ndarray):\n",
    "\n",
    "    'Compare ORB similarity between two images'\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "    kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    # finding how many regions are there\n",
    "\n",
    "    sim_reg = list(filter(lambda x: x.distance < 50, matches))\n",
    "    if len(matches) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(list(sim_reg))/len(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rot_based_on_ref_img(\n",
    "    ref_img:Union[np.ndarray, Image.Image], # Image will be used refeernece image\n",
    "    tst_img:Union[np.ndarray, Image.Image], # Image where searched for key points\n",
    "    show_m:bool=True # whether to show matching points of two images\n",
    "    )->np.ndarray:\n",
    "    'Rotate the tst_img based on ref image, find key points in ref image and then rotate tst_img'\n",
    "    if isinstance(ref_img, Image.Image):\n",
    "        ref_img = np.array(ref_img)\n",
    "    \n",
    "    if isinstance(tst_img, Image.Image):\n",
    "        tst_img = np.array(tst_img)\n",
    "    orb = cv2.ORB_create(50)\n",
    "    # find the keypoints and descriptors with orb\n",
    "    kp1, des1 = orb.detectAndCompute(tst_img, None)  #kp1 --> list of keypoints\n",
    "    kp2, des2 = orb.detectAndCompute(ref_img, None)\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    #Match descriptors.\n",
    "    matches = matcher.match(des1, des2, None) \n",
    "    #Creates a list of all matches, just like keypoints\n",
    "    img3 = cv2.drawMatches(tst_img,kp1, ref_img, kp2, matches[:10], None)\n",
    "    if show_m: show_(img3)\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)  \n",
    "    #Prints empty array of size equal to (matches, 2)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = kp1[match.queryIdx].pt    #gives index of the descriptor in the list of query descriptors\n",
    "        points2[i, :] = kp2[match.trainIdx].pt    #gives index of the descriptor in the list of train descriptors\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "    if len(tst_img.shape) > 2:\n",
    "        height, width, ch = ref_img.shape\n",
    "    else:\n",
    "        height, width = ref_img.shape\n",
    "    \n",
    "    im1Reg = cv2.warpPerspective(tst_img, h, (width, height)) \n",
    "    return im1Reg\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def frm_cntr_to_bbox(cntr):\n",
    "    x,y,w,h = cv2.boundingRect(cntr)\n",
    "    return x,y,w,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def foo(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export('00_core.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
